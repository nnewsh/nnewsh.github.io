---
title: "LSTM for Population Forecasting in R"
output:
  html_document:
    css: "C:/Users/niall/Documents/PhD/Projects/Miguel Mexico Project/Colmex Seminar/style.css" 
---

<div id="title-block">
  <div class="author">Niall Newsham</div>
  <div class="twitter-link"><a href="https://x.com/nnewsh_" target="_blank">@nnewsh_</a></div>
  <div class="date">2024-05-23</div>
  
</div>

This website hosts the materials for the demonstration “Using Long-Short Term Memory Models to Forecast Population in R”. The materials here were prepared by Dr Niall Newsham for demonstration at El Colegio De Mexico in June 2024. The code demonstrated here is derived from <u>_Newsham, N., Kumar, P., Cabrera-Arnau, C,. Rowe, F., 2024. A Long Short-Term Memory Forecast of sub-National Population Change Across Europe_</u>, accessible [here](https://osf.io/preprints/socarxiv/9azqy). 

<br>
<h2> Introduction </h2>

This course demonstrates the capabilities of Long-Short Term Memory (LSTM) models for time series forecasting in R. It provides a framework to build and implement an LSTM model for a specific use case in population forecasting, though its principles can be applied in other time series prediction. LSTMs possess an advanced ability, through deep learning architectures, to understand complex temporal dependencies in longitudinal data. In real-life applications, they power predictive text AI technologies, or financial forecast models. Their ability to process and remember relevant information in an input sequence of data makes them well-suited for predictive functions, including population forecasting. Already, studies have demonstrated their potential in predicting population outcomes (Riiman 2019, Wang & Lee 2021, Grossman 2023), highlighting more accurate estimates than traditional approaches under certain model configurations. 

This course describes the process of building and implementing a population forecast using LSTM machine learning models.
The LSTM model produces a one year ahead forecast of population size, using data from the [WorldPop](https://hub.worldpop.org/geodata/listing?id=75) organisation (Stevens et al., 2015). The input data consists of a time series of annual population values (2000 - 2019) for a total of 9,054 municipalities of the UK. The LSTM is designed to learn of temporal dependencies in this full set of input data, and consider it in predictions of population size in 2020 for individual areas. 

<br>
<hr>

<div id="title-block">
  <h2>Prerequisites</h2>
  
</div>
<br>

This demonstration makes use of Python binding packages, [Keras](https://keras.io/) and [tensorflow](https://tensorflow.rstudio.com/) to facilitate the LSTM forecast in R. These can be installed by following instructions [here](https://tensorflow.rstudio.com/install/). 
NOTE: these binding packages also require the installation of Python version 3.1.0, later versions of Python may be incompatible. 

The following R packages are also essential for the demonstration: 

<u>tensorflow</u> - Binding package, backend for machine learning

<u>keras</u> - Binding package, neural network API

<u>tibble</u> - Data frame manipulation

<u>tidyverse</u> - Data wrangling and manipulation

<u>readr</u> - Reading csv files

These can be installed by running the following into the R console window: 
<br>

```{r eval=FALSE}

install.packages("tensorflow")

install.packages("keras")

install.packages("tibble")

install.packages("tidyverse")

install.packages("readr")

```

and loaded into our current R session using: 
<br>

```{r message = FALSE, warning = FALSE}

library(tensorflow)
library(keras)
library(tibble)
library(tidyverse)
library(readr)

```

<br>
<hr>

<div id="title-block">
  <h2>Data Processing</h2>
  
</div>


The LSTM models can be adapted to forecast a time-series of data. They require for input data to satisfy the following requirements: 

 - Input data to be a complete and consistent time-series with no missing values or truncation
 - Input data to be normalized, which helps to stabilize training
 - Input data to be partitioned into sequences, containing chunks of the full time series. 
 - Input sequence to be partitioned into two distinct sets; a training set containing 80% of the time-series values, and    a  test sets comprising the remaining 20%.  
 - Input data to be formatted as a 3D array, with dimensions samples, time steps, features. In our case, these             correspond to areas (samples), sequence of input data (time steps), and number of variables (features).
 
The majority of the code deals with addressing these assumptions, using a variety of data wrangling techniques. 
 
The first step is to download the time series data, available [here](https://github.com/nnewsh/nnewsh.github.io)
It contains a spatial panel dataset comprising annual population size estimates for municipalities in the UK

<br>
```{r setup, include=FALSE}
# Set the root directory for all chunks
knitr::opts_knit$set(root.dir = "C:/Users/niall/Downloads/")
```

```{r}
#Read in our spatial panel of 20 years (rows) and x areas (columns)
Data <- read.csv("UK.csv")


#Inspect our data:
head(Data, 10)
```

<br>
```{r}

#Define the number of areas and variables - essential for subsequent data wrangling and LSTM model:
num_areas <- nrow(Data)
num_variables <- 1

#Convert spatial panel data to a long format matrix
Data.L <- Data %>% 
  pivot_longer(cols = starts_with('X2'), names_to = "Year", values_to = "Population") %>% 
  mutate(Year = as.numeric(sub("X", "", Year)))

data <- data.matrix(Data.L[, c(4)])

# Add row names for identification
row_names <- paste0("Area", rep(1:num_areas, each = 21), ".", rep(2000:2020, times = num_areas))
rownames(data) <- row_names

head(data, 25)

#Clear dataframes not in use
rm(Data, Data.L, row_names)

```
<br>

Our first data manipulation task involves normalising our time series. This helps the model understand temporal dependencies in input data during training. 

Normalize each feature (variable) independently to have a mean of 0.
Normalize each feature (variable) independently to have a standard deviation of 1.

It is important that the mean and standard deviation correspond to the training data only. We want to avoid using any values corresponding to the test data to avoid data leakage, where information from the test set influences the model during training. This can result in overly optimistic performance estimates and potentially poor generalization to new, unseen data.

To achieve this, we first create a new dataframe containing only the training data. This dataframe will include all of our time series data except for the last 6 values. In this case, these 6 values correspond to the unseen data that is used to make the forecast (sliding window sequence size = 5, plus the target value):

Here, this new dataframe is called _train_ and it is created by first creating an index with the same dimensions as our timeseries consisting of values 1 and 2. A value of 1 corresponds to the time series values that will be allocated to the training dataset, and 2 corresponds to the time series values that are reserved for testing.

<br>
```{r}

index <- rep(rep(c(1, 2), c(15, 6)), length.out = nrow(data))
head(index, 21)

train <- data[index ==1, , drop = FALSE]
head(train, 15)

```

Next, we normalise the full time series (including train and test data) using the mean and standard deviation of the new *training set*:
```{r}

mean <- apply(train, 2, mean)
std <- apply(train, 2, sd)

normalized_data <- scale(data, center = mean, scale = std)

```

```{r}
# Remove dataframes not in use
rm(train, data, index)
```

<br>
<hr>

<div id="title-block">
  <h2>Sequence Creation</h2>
  
</div>

Our next data manipulation task involves creating the sequences of our input time-series. LSTMs process the full input data by reading these sequences one by one, in what is known as a sliding window approach. Here, our window length is set to 5, meaning that each sequence consists of 5 annual population values. The first sequence therefore corresponds to the first 5 values in our time series, or the first 5 annual population values (2000 - 2004).

Each sequence is accompanied by a target value, which is the next value in the time series, or the 6th value. Recall that in backpropagation, the LSTM model estimates the next value in the sequence and uses the error (a function of the difference between the estimate and actual value) to adjust its weights and improve subsequent estimations. In our case, given a sequence length of 5, the target value corresponds to 6th value from the beginning of the sequence.  

To better understand how these sequences are used by the LSTM model. Consider the formulation of our forecast task: 
Given a sequence of data corresponding to 5 annual population values, and sampled every one timestep (moved iteratively one year at a time), can we predict population size in the next year? 

<br>

![Sliding Window Process](https://raw.githubusercontent.com/nnewsh/nnewsh.github.io/main/sliding%20window.png){ width=75% }


```{r}

#We first establish the sliding window sequences of length 5:
window_size <- 5

#Create empty lists to store sequences and targets for each area
#Results in
sequences <- vector("list", num_areas)
targets <- vector("list", num_areas)

#Given that each sequence consists of 5 consecutive population values and is accompanied by a target, the 6th value, *each area contains 16 sequences

for (i in 1:num_areas) {
#Calculate the starting row index for the current area
  start_row <- (i - 1) * 21 + 1
  
#Extract the population data for the current area
  area_data <- normalized_data[start_row:(start_row + 21 - 1), "Population"]
  
#Create empty matrices to store sequences and targets for the current area
  area_sequences <- matrix(0, nrow = length(area_data) - window_size, ncol = window_size)
  area_targets <- matrix(0, nrow = length(area_data) - window_size, ncol = 1)
  
#Generate sequences and targets for the current area
  for (j in 1:(length(area_data) - window_size)) {
    # Extract the sliding window sequence from the area data
    area_sequences[j, ] <- as.vector(t(area_data[j:(j + window_size - 1)]))
    
#Extract the target value for the next time step
    area_targets[j, ] <- area_data[j + window_size]
  }
  
#Store the sequences and targets in the respective lists
  sequences[[i]] <- area_sequences
  targets[[i]] <- area_targets
  
}

# Combine sequences and targets for all areas
all_sequences <- do.call(rbind, sequences)
all_targets <- do.call(rbind, targets)


```
 
<br>
We can confirm this is correct by comparing all_sequences and all_targets with our normalized data.
We expect for the 16th and final sequence to consist of data from 2015-2019 with a target corresponding to 2020: 
 
```{r}

print(normalized_data[16:21])

print(all_sequences[16, ])
print(all_targets[16, ])
```
<br>
<hr>
<div id="title-block">
  <h2>Train/Test Split</h2>
  
</div>


The next data manipulation tasks involve partitioning our sequences and targets into training and test sets. 
The LSTM will learn of dependencies in the training data and apply these to the test set. 

As a rule of thumb, the a training set containing should contain 80% of the time-series values, with the test set comprising the remaining 20%. We achieve this in a similar way to our earlier normalizing task, by creating an index of values. This time, we apply the index to our all_sequences and all_targets dataframes, with the first 15 sequences used for training, and the final sequence reserved for testing:

<br>
```{r}

index <- rep(rep(c(1, 2), c(15, 1)), length.out = nrow(all_sequences))

train_data <- all_sequences[index == 1, , drop = FALSE]
train_targets <- all_targets[index == 1, , drop = FALSE]

test_data <- all_sequences[index == 2, , drop = FALSE]
test_targets <- all_targets[index == 2, , drop = FALSE]

```

Again, we can check the this has worked correctly by printing the newly created dataframes. We are expecting that our train_data to contain the first 15 sequences, corresponding to population data from 2000-2018, and our test data includes the final sequence corresponding to 2015-2019 data. The accompanying targets should contain the next value in the time-series, so that test_targets corresponds to the normalized population value at year 2020: 
<br>
```{r}

print(train_data[15, ])
print(train_targets[15, ])

print(test_data[1, ])
print(test_targets[1, ])
```
<br>
The final data manipulation tasks involves reshaping our train and test data into a format that is readable by an LSTM model. LSTM's require input data to be formatted as a 3D array, with dimensions samples, time steps, features. In our case, these correspond to the number sequences per area (samples), sequence of input data (time steps), and number of variables (features).

This is accomplished by: 

<br>
```{r}

train_data <- array_reshape(train_data, c(dim(train_data)[1], window_size, 1))

dim(train_data)
head(train_data)


```
<br>
We see that each row represents a sequence of 5 values (5 columns) from our data. Since we only have one variable, the third dimension has only one layer. 

<br>
```{r}

test_data <- array_reshape(test_data, c(dim(test_data)[1], window_size, 1))

dim(test_data)
head(test_data)

```
<br>
<hr>
<div id="title-block">
  <h2>Training an LSTM</h2>
  
</div>

Thus far our entire code has been working to get the input data into a way that is readable by the LSTM model. 
Now that we have satisfied all data requirements, we can now build an LSTM model that will forecast a future population value. This is fairly straightforward to accomplish by using the Keras and Tensorflow binding packages. 

From this point, we can tailor various parameters and the overall structure of our LSTM model. These decisions have an influence on the predictive ability, or accuracy, of our forecasts. The following parameters can be altered: 

- Number of layers: The number of LSTM layers in the model architecture.

- Number of LSTM units: The number of memory units or cells in the LSTM layer.

- Activation function: The activation function used within the LSTM layer.

- Number of epochs: The number of times the entire dataset is passed forward and backward through the LSTM model.

- Batch size: The number of samples propagated through the network before the model's weights are updated.

- Learning rate: The rate at which the model adjusts its weights during training.

- Loss function: The function used to calculate the difference between predicted and actual values.

- Optimizer: The algorithm used to minimize the loss function, such as Stochastic Gradient Descent (SGD) or Adam.

It is recommended to assess the effect of these changing parameters by running multiple test models. This is facilitated by the Hyperparameter Tuner (URL___). For this demonstration, the optimal parameters have already been identified in this way, though certain parameters are selected to reduce the run time of our LSTM model. Here we design a simple LSTM model that is relatively quick to train whilst still demonstrating the capabilities of this approach. Specifically, we will set the following parameters: 

- Number of layers: 1 LSTM layer followed by a dense layer

- Number of LSTM units: 32

- Activation function: Relu 

- Number of epochs: 10

- Batch size: no. training sequences per each area (15)

- Learning rate: 0.001

- Loss function: Mean Squared Error

- Optimizer: Adam

<br>
```{r}

set.seed(123) 
epochs <- 5
batch_size <- 15

#Build the LSTM model
model <- keras_model_sequential()
model %>% 
    layer_lstm(units = 32, input_shape = c(window_size, 1), activation = 'relu') %>%
    layer_dense(units = 1)
  
#Compile the model
model %>% compile(
  loss = "mean_squared_error",
  optimizer = optimizer_adam(learning_rate = 0.001))
  
#Train the model
  history <- model %>% fit( 
    train_data, train_targets,
    epochs = epochs,
    batch_size = batch_size,
    validation_data = list(test_data, test_targets)
  )
  
```
<br>
<hr>
<div id="title-block">
  <h2>Forecasting Population Using an LSTM</h2>
  
</div>

We are now ready to produce a population forecast for the next annual value in our time series, corresponding to the year 2020. We also compare our forecast with the known population values in 2020 to compute an accuracy metric. We will use the mean and median absolute percentage error (APE) to gauge the average accuracy of our forecasts over each area. APE calculates the absolute percentage difference between the forecasted and actual values, providing insights into the magnitude of errors relative to the actual values.

It is calculated by: 

 |(At-Ft )/At |  ×100 
 
where At is the actual value and Ft is the forecast value.

<br>

```{r}
  
#Forecast the next value in the test_data sequence. Given that test_data contains 2015-2019 data, we are forecasting the population in year 2020. 
  predicted_targets <- model %>% predict(test_data)
  
#Denormalize the predictions
  predicted_targets <- predicted_targets *  std[1] + mean[1]
  
head(predicted_targets)

```
<br>
Finally, we compare our predicted values with the actual 2020 population size figures. We do this by re-reading our original data set, extracting only the columns corresponding to 2020 and area IDs, and merging with our predicted_targets data set. We then use the absolute percentage error formula to produce a metric that captures the difference between predicted and actual 2020 population size values:
<br>

```{r}

actual.values <- read.csv("C:/Users/niall/Downloads/UK.csv")
actual.values <- actual.values[c(1,23)]

Accuracy <- cbind(actual.values,predicted_targets)

Accuracy$APE <- abs((Accuracy$X2020 - Accuracy$predicted_targets) / Accuracy$X2020) * 100

mean(Accuracy$APE)
median(Accuracy$APE)
```